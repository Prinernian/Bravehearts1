{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "class WebScrapper:\n",
    "    \n",
    "    def __init__(self, urls: List[str]) -> None:\n",
    "        self.urls = urls\n",
    "        self.corpus = []\n",
    "        self.doc_id = 1\n",
    "\n",
    "    def start_scraping(self) -> None:\n",
    "        for url in self.urls:\n",
    "            text, title, description = self.scrap_webpage(url)\n",
    "            if text and title:\n",
    "                cleaned_text = self.preprocess_text([text])[0]\n",
    "                doc = {\n",
    "                    'doc_id': str(self.doc_id),\n",
    "                    'title': title,\n",
    "                    'description': description,\n",
    "                    'url': url,\n",
    "                    'lang': 'en',  \n",
    "                    'text': \" \".join(cleaned_text)\n",
    "                }\n",
    "                self.corpus.append(doc)\n",
    "                self.doc_id += 1\n",
    "        self.save_corpus_to_jsonl()\n",
    "\n",
    "    def scrap_webpage(self, url: str) -> (str, str, str):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            title = soup.title.string if soup.title else \"No Title\"\n",
    "\n",
    "            #extract meta description\n",
    "            meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "            description = meta_desc['content'] if meta_desc else \"No Description\"\n",
    "            # Get raw text from the webpage\n",
    "            text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            return text, title, description\n",
    "        else:\n",
    "            print(\"Error: \", response.status_code)\n",
    "            return \"\", \"\"\n",
    "\n",
    "    # Preprocessing method\n",
    "    def preprocess_text(self, documents: List[str]) -> List[List[str]]:\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        def clean_text(text):\n",
    "            # Remove headers\n",
    "            text = re.sub(r'From:.*|Subject:.*|Date:.*|Lines:.*|Reply-To:.*', '', text)\n",
    "\n",
    "            # Lowercase the text\n",
    "            text = text.lower()\n",
    "\n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "            # Remove special characters and numbers\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "            # Tokenize\n",
    "            tokens = text.split()\n",
    "\n",
    "            # Remove stopwords and short tokens\n",
    "            tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        # Process the document and return the cleaned document\n",
    "        return [clean_text(doc) for doc in documents]\n",
    "\n",
    "    # Save the corpus to a JSONL file\n",
    "    def save_corpus_to_jsonl(self) -> None:\n",
    "        with open('web_corpus.jsonl', 'w', encoding='utf-8') as f:\n",
    "            for doc in self.corpus:\n",
    "                json.dump(doc, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = [\n",
    "        \"https://computer.ing.unipi.it/home\",\n",
    "        \"https://computer.ing.unipi.it/ce-lm\",\n",
    "        \"https://computer.ing.unipi.it/ce-lm/how-to-enroll\",\n",
    "        \"https://computer.ing.unipi.it/ce-lm/study-plan-a-y-202324\",\n",
    "        \"https://computer.ing.unipi.it/ce-lm/study-plan-a-y-202223\",\n",
    "        \"https://computer.ing.unipi.it/ce-lm/study-plan-a-y-202122\",\n",
    "        \"https://computer.ing.unipi.it/ce-lm/study-plan-20-21\",\n",
    "        \"https://computer.ing.unipi.it/aide-lm\",\n",
    "        \"https://computer.ing.unipi.it/aide-lm/how-to-enroll\",\n",
    "        \"https://computer.ing.unipi.it/aide-lm/study-plan-a-y-202324\",\n",
    "        \"https://computer.ing.unipi.it/aide-lm/study-plan-a-y-202223\",\n",
    "        \"https://computer.ing.unipi.it/aide-lm/study-plan-a-y-202122\",\n",
    "        \"https://computer.ing.unipi.it/aide-lm/study-plan-20-21\",\n",
    "        \"https://computer.ing.unipi.it/news\"\n",
    "    ]\n",
    "    webscrapper = WebScrapper(urls)\n",
    "    webscrapper.start_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIRCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
